{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value-at-risk calculations\n",
    "\n",
    "The basic idea behind the value-at-risk calculation is that we're going to look at the historical returns of a portfolio of securities and run many simulations to determine the range of returns we can expect from these.  We can then predict, over a given time horizon, what our expected loss is at a given probability, e.g., we might say that there is less than a 10% chance that the portfolio will lose more than $1,000,000.\n",
    "\n",
    "Note that this is a didactic example and consequently makes some simplifying assumptions about the composition of the portfolio (i.e., only long positions in common stocks, so no options, dividends, or short selling) and the behavior of the market (i.e., day-to-day return percentages are normally-distributed and independent).  Do not use this code to guide actual investment decisions!\n",
    "\n",
    "## Basic setup\n",
    "\n",
    "Here we import the `pyspark` module and set up a `SparkSession`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "We'll start by loading the data (from the WikiEOD dataset of freely-available stock closing prices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.load(\"/data/wikieod.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating historical returns\n",
    "\n",
    "We'll use Spark's windowing functions over data frames to determine the percentage change in each security from the previous close to each day's close.  Basically, we'll add a column to our data frame that represents the percentage change from the previous day's close (that is, `lag(\"close\", 1)` when partitioned by ticker symbol and ordered by date) and the current day's close (that is, `col(\"close\")`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lag, col, avg, variance\n",
    "\n",
    "ddf = df.select(\"ticker\", \"date\", \"close\").withColumn(\"change\", (col(\"close\") / lag(\"close\", 1).over(Window.partitionBy(\"ticker\").orderBy(df[\"date\"])) - 1.0) * 100)\n",
    "ddf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characterizing expected return distributions\n",
    "\n",
    "With the range of changes now available, we can calculate our expected returns for each security.  Since this is a simple example, we'll assume that returns are normal (in the real world, you'd want to use a distribution with heavier tails or a more sophisticated technique altogether).  We can calculate the parameters for each security's distribution as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sqrt\n",
    "mv = ddf.groupBy(\"ticker\").agg(avg(\"change\").alias(\"mean\"), sqrt(variance(\"change\")).alias(\"stddev\"))\n",
    "mv.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are only about 3,000 ticker symbols in our data set, we can easily collect these in driver memory for use in our simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_map = mv.rdd.map(lambda r: (r[0], (r[1], r[2]))).collectAsMap()\n",
    "dist_map[\"RHT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting current security prices\n",
    "\n",
    "We'll now identify the latest price for each security in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import first\n",
    "priceDF = ddf.orderBy(\"date\", ascending=False).groupBy(\"ticker\").agg(first(\"close\").alias(\"price\"), first(\"date\").alias(\"date\")).cache()\n",
    "priceDF.show(10)\n",
    "\n",
    "prices = priceDF.rdd.map(lambda r: (r[0], r[1])).collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a simulation\n",
    "\n",
    "We'll now define our simulation.  This involves three steps: \n",
    "\n",
    "1.  We'll start by generating a random portfolio of securities (a map from ticker symbols to values); then, \n",
    "2.  we'll decide how many simulations to run and generate a random seed for each; and, finally\n",
    "3.  we'll actually run the simulation for a given number of training days, updating the value of our portfolio with random returns sampled from the distribution of historical returns.\n",
    "\n",
    "Generating the random portfolio is pretty straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint, seed\n",
    "\n",
    "def random_portfolio(symbols):\n",
    "    result = {}\n",
    "    for s in symbols:\n",
    "        result[s] = prices[s] * (randint(1, 1000) * 11)\n",
    "    return result\n",
    "\n",
    "def portfolio_value(pf):\n",
    "    return sum([v for v in pf.values()])\n",
    "\n",
    "seed(0xdea110c8)\n",
    "\n",
    "portfolio = random_portfolio(ddf.select(\"ticker\").distinct().sample(True, 0.01, 0xdea110c8).rdd.map(lambda r: r[0]).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is generating a collection of random seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seeds(count):\n",
    "    return [randint(0, 1 << 32 - 1) for i in range(count)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a single step of our simulation (and, subsequently, a whole run of the simulation) next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simstep(pf, params, prng):\n",
    "    def daily_return(sym):\n",
    "        mean, stddev = params[sym]\n",
    "        change = (prng.normalvariate(mean, stddev) + 100) / 100.0\n",
    "        return change\n",
    "    return dict([(s, daily_return(s) * v) for s, v in pf.items()])\n",
    "\n",
    "def simulate(seed, pf, params, days):\n",
    "    from random import Random\n",
    "    prng = Random()\n",
    "    prng.seed(seed)\n",
    "    pf = pf.copy()\n",
    "    \n",
    "    for day in range(days):\n",
    "        pf = simstep(pf, params, prng)\n",
    "    return pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need to actually run the simulation.  For each seed, we'll spawn a Spark job to simulate 5 days of activity on our portfolio and then return the total dollar value of our gain or loss at the end of the period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "days_to_simulate = 5\n",
    "simulation_count = 10000\n",
    "\n",
    "sc = spark.sparkContext\n",
    "seed_rdd = sc.parallelize(seeds(simulation_count))\n",
    "bparams = sc.broadcast(dist_map)\n",
    "bpf = sc.broadcast(portfolio)\n",
    "initial_value = portfolio_value(portfolio)\n",
    "\n",
    "results = seed_rdd.map(lambda s: portfolio_value(simulate(s, bpf.value, bparams.value, days_to_simulate)) - initial_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "simulated_results = list(zip(results.collect(), seed_rdd.collect()))\n",
    "simulated_values = [v for (v, _) in simulated_results]\n",
    "simulated_values.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "_ = sns.distplot(simulated_values, kde=False).set(xlabel=\"$ change\", ylabel=\"simulation count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot only every 20th simulation result (unless that would leave us with fewer \n",
    "# than 50 elements) to avoid generating a huge figure\n",
    "plotevery = len(simulated_values) > 1000 and 20 or 1\n",
    "\n",
    "xvals = [float(i) / len(simulated_values) for i in range(len(simulated_values))]\n",
    "ax = sns.tsplot(np.array(simulated_values[::plotevery]), np.array(xvals[::plotevery]))\n",
    "\n",
    "# add ticks for every ten percent\n",
    "ax.get_xaxis().set_ticks([i * 0.1 for i in range(11)])\n",
    "_ = ax.get_xaxis().set_ticklabels([\"%d%%\" % (i * 10) for i in range(11)])\n",
    "\n",
    "ax.set(xlabel=\"cumulative percentage of simulations with at least this change\", ylabel=\"gain or loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the market historically trends up, we have a better than even chance of not losing money in our simulation.  We can see the 5% value at risk over our time horizon by taking the corresponding element out of the sorted simulation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5% value at risk\n",
    "percentage_var = 0.05\n",
    "\n",
    "simulated_values[int(len(simulated_values) * percentage_var)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing random walks\n",
    "\n",
    "Finally, let's look at some example simulation runs to see how the portfolio value changes over time.  We'll take the runs with the best and worst returns and also the runs at each decile.  To visualize our simulation, we'll need a slightly different `simulate` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate_with_history(seed, pf, params, days):\n",
    "    from random import Random\n",
    "    prng = Random()\n",
    "    prng.seed(seed)\n",
    "    pf = pf.copy()\n",
    "    values = [portfolio_value(pf)]\n",
    "    \n",
    "    for day in range(days):\n",
    "        pf = simstep(pf, params, prng)\n",
    "        values.append(portfolio_value(pf))\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now repeat the simulation on eleven of our seeds from the earlier run, collecting history for each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simulated_results.sort()\n",
    "\n",
    "# choose the least result, the 10%ile, the 20%ile, ..., the 90%ile, and the greatest result\n",
    "eleven_results = [simulated_results[int((len(simulated_results) - 1) * i / 10)] for i in range(11)]\n",
    "eleven_seeds = sc.parallelize([seed for (_, seed) in eleven_results])\n",
    "walks = eleven_seeds.map(lambda s: simulate_with_history(s, bpf.value, bparams.value, 5))\n",
    "\n",
    "walk_results = walks.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in walk_results:\n",
    "    ax = sns.tsplot(c)\n",
    "\n",
    "ax.set(xlabel=\"day of simulation\", ylabel=\"total portfolio value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting more realistic results\n",
    "\n",
    "Of course, most real-world stock returns aren't normally distributed.  To make a more interesting simulation, we can try to find a distribution that better models the returns we've observed.\n",
    "\n",
    "We'll start by looking at the actual distributions of returns.  The Seaborn `distplot` function will plot a histogram of returns and fit a [kernel density estimate](https://en.wikipedia.org/wiki/Kernel_density_estimation) to the observations.  To see this in action, we'll try it out first with a single security:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdist = ddf.filter(ddf[\"ticker\"] == \"RHT\").select(\"change\").rdd.map(lambda r: r[\"change\"]).filter(lambda c: c is not None).collect()\n",
    "sns.distplot(rdist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the distribution for this particular security doesn't look all that normal.  While there are various [statistical techniques to quantify](https://en.wikipedia.org/wiki/Goodness_of_fit) just how bad of a fit the normal distribution is for our observations (see in particular [this one](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test)), `distplot` will let us visually compare an empirically-observed distribution to an ideal distribution in order to see how well the latter fits the former.  We can do this to see just how far this distribution is from normal.\n",
    "\n",
    "We'll first import a library of models and then plot the expected return distribution if returns were normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "sns.distplot(rdist, fit=stats.norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the normal distribution (black line) is not a great fit for the kernel density estimate of our observations (blue line).  While we'd want a more rigorous way to choose models in a production application, looking at the returns from multiple securities is a good way to continue our exploration of these data.  We'll do that now.\n",
    "\n",
    "First, we'll choose a small number of securities at random:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symbols = ddf.select(\"ticker\").distinct().sample(True, 0.001).rdd.map(lambda l: l[\"ticker\"]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use Spark to generate a data frame with all of the returns for each symbol in `symbols`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs = ddf.filter(ddf[\"ticker\"].isin(symbols)).select(\"ticker\", \"change\").dropna()\n",
    "sampled_returns = dfs.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack what's happening here:\n",
    "\n",
    "1.  We start with `ddf`, our `DataFrame` of stock returns.\n",
    "2.  We're going to filter out any row that isn't for a symbol we care about.  In SQL, this would look like `SELECT ticker, change WHERE ticker IN ...`; in the Spark data frame DSL we use `isin`, like this:  `ddf[\"ticker\"].isin(symbols)`.\n",
    "3.  The return for the first day we have data for each ticker symbol will be null (by definition), and we don't want to have to deal with those in our plot functions.  The `dropna()` function is a convenient way to do this.\n",
    "4.  Finally, we're going to collect the Spark data frame in memory and convert it to a Pandas data frame, which will make it easier to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_fit(distribution=\"norm\"):\n",
    "    d = getattr(stats, distribution)\n",
    "    g = sns.FacetGrid(sampled_returns, row=\"ticker\", sharex=False, sharey=False, aspect=3)\n",
    "    _ = g.map(sns.distplot, \"change\", fit=d)\n",
    "\n",
    "show_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can re-run this experiment several times to see the results with a different set of sampled ticker symbols or a different probability distribution.\n",
    "\n",
    "We have the intuition that the normal distribution isn't the best option, and it's easy to look for a better one.  Start by seeing [what distributions are available in the SciPy `stats` module](https://docs.scipy.org/doc/scipy/reference/stats.html) or try to find the best distribution that fits the data by choosing various distributions from the drop down menu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "distributions = [key for key, value in stats.__dict__.items() if callable(getattr(value, \"fit\", None))]\n",
    "distributions.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(show_fit, distribution = distributions);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can try some different distributions and see if one looks like a good option. What distributions seem to be good fits? With some more exploratory work, you're able to turn this example code into a more realistic simulation by using some more suitable distribution in the `daily_return` method.\n",
    "\n",
    "# Exercises\n",
    "\n",
    "1.  Find a distribution that is a better fit for the stock returns data.\n",
    "2.  Use this distribution to fit models to each ticker symbol and develop a new `simulate` method to sample from these distributions rather than from normal distributions.  Compare the results of this simulation to the results of the simulation\n",
    "3.  &starf; Implement a goodness-of-fit test of your choice.\n",
    "4.  Using the test you developed above, evaluate the distribution fits for every symbol.  Are there any particularly bad fits?\n",
    "5.  &starf; Identify some additional distributions to use for modeling security returns that don't work well with the distribution you identified above.  Build a more sophisticated simulation that chooses one of several possible distributions for each security to find the best fit.\n",
    "6.  &starf; Past performance is no guarantee of future results, but it's one of the best indicators we have.  Nevertheless, we should be careful.  What are some potential pitfalls and limitations of this general approach?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
